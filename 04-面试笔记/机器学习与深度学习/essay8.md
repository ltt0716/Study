# 过拟合
- 如果模型过于复杂，它会把训练数据里的噪声和随机波动都学进去，而不是学习数据背后真正的规律。这会导致模型在训练集上误差很低，但在新数据（测试集）上表现很差。
- 过拟合在线性回归中是什么样的？
通常表现为某些特征的权重 w 变得非常大。这使得模型对这些特征的微小变化极为敏感，失去了泛化能力。
#  L2 正则化项 (L2 Regularization Term)
- 解决过拟合问题
- 解决方法：增加惩罚项：
为了防止权重 w 变得过大，我们在原来的成本函数（衡量预测误差）后面，增加了一个“惩罚项”。
![L2 Regularization](.\images\image.png)

# 训练集 验证集 测试集
- **训练集 (Training Set)**  
作用： 模型直接从这些数据中学习，通过反向传播不断更新权重 w 和偏置 b，以最小化损失函数。

- **验证集 (Validation Set)**  
作用： 不参与模型的权重更新。它的唯一作用是：在训练过程中，定期评估不同“学习策略”（即超参数）下的模型表现如何。
- **测试集 (Test Set)**  
作用： 完全独立的数据，只在模型最终选定后使用一次，用来评估模型的最终泛化能力。

# 验证集的两种方法
## 简单留出验证集 (Hold-out Validation) 
这是在深度学习中最常见、最实用的方法，因为它的计算成本相对较低。实现步骤如下：
1. **数据划分**  
在项目开始时，一次性将全部数据划分为三部分：训练集、验证集、测试集。
常见的比例是 70% / 15% / 15% 或 80% / 10% / 10%。如果数据量巨大（百万级以上），验证集和测试集的比例可以更小。
2. **超参数调优循环**  
你想比较几组不同的超参数，比如：  
模型A： 学习率=0.01，2个隐藏层，使用Adam优化器。
模型B： 学习率=0.001，3个隐藏层，使用Adam优化器。
模型C： 学习率=0.01，2个隐藏层，使用SGD优化器。
对每一个模型配置，执行以下操作：  
- 在训练集上训练模型。
- 每训练一个或几个 epoch（轮次），就在验证集上评估一次模型的性能（如准确率、损失值）。
- 记录下该模型在验证集上的最佳表现。同时，这也是早停法 (Early Stopping) 的依据：如果模型在验证集上的表现连续多轮不再提升，就可以提前停止训练，防止过拟合。
3. **选择最佳模型**
所有模型（A, B, C）都跑完后，比较它们各自在验证集上的最佳表现。
假设模型B在验证集上的准确率最高，那么你就认为 学习率=0.001，3个隐藏层，使用Adam优化器 是最佳的超参数组合。
4. 最终评估：
将选出的最佳模型（模型B），在从未见过的测试集上进行一次最终的评估。
这个成绩，才是你可以对外报告的、代表模型真实泛化能力的成绩。
## K-折交叉验证 (K-Fold Cross-Validation) - 更可靠但成本高
这是传统机器学习中非常标准的方法。在深度学习中，由于训练耗时太长，它通常只在数据集较小或对评估结果的可靠性要求极高的场景下使用。实现步骤如下：  
1. **数据划分**  
首先，将测试集完全分出去，锁起来不用。将剩下的数据（训练集 + 验证集）合并，然后把它平均分成 K 份（比如 K=5 或 K=10）。
2. **K-折交叉验证循环**  
你想测试一组超参数（比如上面提到的模型A的配置）。
进行 K 次训练和验证的循环：  
第1轮： 用第1份数据做验证集，剩下的 K-1 份做训练集。训练模型，记录在第1份验证集上的分数。  
第2轮： 用第2份数据做验证集，剩下的 K-1 份做训练集。训练模型，记录在第2份验证集上的分数。  
...  
第K轮： 用第K份数据做验证集，剩下的 K-1 份做训练集。训练模型，记录在第K份验证集上的分数。
3. **计算平均性能**  
将 K 次得到的验证分数（比如准确率）求一个平均值。这个平均值，就是模型A这套超参数配置的最终评估分数。它比单次验证的结果更稳定、更可靠，因为它利用了所有数据进行验证。
4. **选择最佳模型**  
对你想要尝试的每一套超参数组合（模型A, B, C...），都完整地重复一遍上面的步骤2和3。最后，比较哪一套超参数组合得到的平均验证分数最高。
5. **最终评估**  
选定最佳的超参数组合后，通常会用这套配置在全部的非测试集数据上重新训练一个最终模型。然后用这个最终模型在被锁起来的测试集上进行评估。