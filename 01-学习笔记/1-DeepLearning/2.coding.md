# **权重 (weights) 的随机初始化**。

```python
X_train = np.array([[1.0], [2.0]], dtype=np.float32)           
Y_train = np.array([[300.0], [500.0]], dtype=np.float32)      
```
![alt text](image-38.png)


这个 `-0.43` 不是凭空出现的，它是线性层 (linear_layer) 进行数学计算后得到的结果。这个计算遵循一个经典的公式：

**`输出 = (输入 * 权重) + 偏置`**
(`output = (input * weight) + bias`)

现在，我们来一步步分解，看看在您的例子中这几个值分别是什么：

### 1. 输入 (Input)

这个很明确，就是您传入函数的值：
*   `X_train[0]` 是 `[1.0]`。
*   经过 `.reshape(1,1)` 后，输入变成了 `[[1.0]]`。
*   所以，**输入值是 `1.0`**。

### 2. 权重 (Weight)

这是理解 `-0.43` 来源的**关键**！
*   看图片中的文字描述：“**These weights are randomly initialized to small numbers...**” (这些权重被随机初始化为一些小的数值)。
*   当您第一次调用 `linear_layer` 时，TensorFlow 就在内部创建了它的权重变量 `w`。它不知道正确的权重应该是多少，所以它就**随机猜一个很小的数**作为起点。
*   在您**这一次**运行中，TensorFlow 随机选择的那个数字**恰好就是 `-0.43`**。
*   所以，**权重 `w` 的值是 `-0.43`**。

> **重要提示**：如果您从头开始重新启动并运行这个 Notebook，权重会再次被随机初始化，您可能会得到一个完全不同的数字，比如 `0.15` 或者 `-0.81` 等等。

### 3. 偏置 (Bias)

*   图片中的文字描述说：“**...and the bias defaults to being initialized to zero.**” (...并且偏置默认被初始化为零)。
*   所以，**偏置 `b` 的值是 `0`**。

### 让我们把它们代入公式

现在我们有了所有的部分，可以进行计算了：

`输出 = (输入 * 权重) + 偏置`
`a1 = (1.0 * -0.43) + 0`
`a1 = -0.43`

**这就是 `-0.43` 这个数字的由来。**

### 总结与延伸

您可以把神经网络的训练过程想象成这样：

1.  **随机猜测**：网络最开始对权重（比如这里的 `-0.43`）进行一次完全随机的猜测。
2.  **计算结果**：用这个随机的权重进行一次计算，得到一个初步的结果（比如 `-0.43`）。
3.  **比较差距**：拿这个结果 (`-0.43`) 和我们期望的真实目标 (`Y_train` 中的 `300.0`) 进行比较，发现差距巨大。
4.  **学习和调整**：神经网络通过一个叫做“反向传播”和“梯度下降”的过程，微调它的权重，让下一次的计算结果能离真实目标更近一点。
5.  **重复**：不断重复第 2-4 步成千上万次，权重会从最初随机的 `-0.43` 被逐渐调整到一个最优的值，使得输入 `1.0` 时，输出能非常接近 `300.0`。
   

# **布尔索引 (Boolean Indexing)** 或 **掩码 (Masking)**

### **如何根据标签（或条件）来筛选和可视化数据**  
- 示例代码如下：
```
# 创建数据集：特征和标签
X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)  
Y_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1) 

pos = Y_train == 1
neg = Y_train == 0
X_train[pos]
```

### **第二部分：最关键的逻辑 - 布尔索引**

这是您可能感到困惑的核心。这个操作叫做 **布尔索引 (Boolean Indexing)** 或 **掩码 (Masking)**。

```python
pos = Y_train == 1
neg = Y_train == 0
```

1.  `pos = Y_train == 1` 是什么意思？
    *   它不是一个普通的赋值语句，而是一个**比较操作**。
    *   NumPy会逐个元素地将 `Y_train` 中的值与 `1` 进行比较。
    *   如果元素**等于** `1`，就在对应位置返回 `True`。
    *   如果元素**不等于** `1`，就在对应位置返回 `False`。

    我们来看一下具体的计算过程：
    *   `Y_train` 是: `[[0.], [0.], [0.], [1.], [1.], [1.]]`
    *   `Y_train == 1` 的结果是: `[[False], [False], [False], [True], [True], [True]]`
    *   所以，变量 `pos` 现在就是一个由 `True` 和 `False` 组成的**布尔数组 (boolean array)**。

2.  `X_train[pos]` 是什么意思？
    *   这行代码的意思是：“**从 `X_train` 中，只取出那些在 `pos` 数组中对应位置为 `True` 的元素**”。
    *   `pos` 数组在第3、4、5个位置（索引从0开始）是 `True`。
    *   所以，它就会去 `X_train` 中取出第3、4、5个位置的元素。
    *   `X_train` 中对应位置的元素是 `3.`、`4.` 和 `5.`。
    *   因此，`X_train[pos]` 的输出就是 `array([3., 4., 5.])`。

同理，`neg = Y_train == 0` 会得到 `[[True], [True], [True], [False], [False], [False]]`，所以 `X_train[neg]` 会取出 `X_train` 中前三个元素 `[0., 1., 2.]`。

---

### **将逻辑应用于绘图**

现在我们理解了 `pos` 和 `neg` 是两个“筛选器”，再来看绘图代码就非常清晰了。

1.  **绘制正例 (Positive Cases)**:
    ```python
    ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label="y=1")
    ```
    *   `X_train[pos]`: 只取出所有 `y=1` 的点的 x 坐标 `[3, 4, 5]`。
    *   `Y_train[pos]`: 只取出所有 `y=1` 的点的 y 坐标 `[1, 1, 1]`。
    *   所以这行代码画出了三个点：(3, 1), (4, 1), (5, 1)。
    *   `marker='x'` 表示用 'x' 形状来画，`c='red'` 表示用红色。

2.  **绘制负例 (Negative Cases)**:
    ```python
    ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label="y=0", ...)
    ```
    *   `X_train[neg]`: 只取出所有 `y=0` 的点的 x 坐标 `[0, 1, 2]`。
    *   `Y_train[neg]`: 只取出所有 `y=0` 的点的 y 坐标 `[0, 0, 0]`。
    *   所以这行代码画出了三个点：(0, 0), (1, 0), (2, 0)。
    *   `marker='o'` 表示用 'o' 形状来画，颜色是蓝色。

![alt text](image-39.png)

--- 

# `keras.layers.Dense` 参数解析

`keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')`

这行代码在 Keras 中定义了一个全连接层（Dense layer）。下面是每个参数的详细解释：

*   **`Dense`**: `Dense` 是 Keras 中用来创建全连接层的类。在全连接层中，每个神经元都与前一层的所有神经元相连接。 这个层实现了一个基本的数学运算：`output = activation(dot(input, kernel) + bias)`，其中 `kernel` 是权重矩阵，`bias` 是偏置向量，`activation` 是激活函数。

*   **`1` (第一个参数 `units`)**: 这个参数定义了该层中神经元（或称为单元）的数量。 在这个例子中，`units=1` 表示该层只有一个神经元。这个值决定了该层的输出空间的维度。

*   **`input_dim=1`**: 这个参数指定了输入数据的维度。 `input_dim=1` 意味着输入到这一层的数据是一个一维向量。**注意：** `input_dim` 或 `input_shape` 参数通常只需要在模型的第一层设置，因为后续的层可以自动推断出输入的形状。

*   **`activation = 'sigmoid'`**: 这个参数设置了该层神经元的激活函数。激活函数的作用是向网络中引入非线性，使得神经网络能够学习和表示更复杂的模式。
    *   **Sigmoid 函数**: Sigmoid 函数会将任何实数值压缩到 0 和 1 之间。 这使得它特别适用于二元分类问题的输出层，因为输出可以被解释为概率。

*   **`name='L1'`**: 这个参数为该层指定了一个唯一的名称 "L1"。在构建复杂的网络模型时，为层命名是一个好习惯，这有助于后续的模型可视化、调试和层操作。

--- 

# 特征归一化 Feature Normalization
```
print(f"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}")
print(f"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}")
norm_l = tf.keras.layers.Normalization(axis=-1)
norm_l.adapt(X)  # learns mean, variance
Xn = norm_l(X)
print(f"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}")
print(f"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}")
```
这段代码的核心作用是对数据进行 **特征归一化 (Feature Normalization)**，具体来说是 **标准化 (Standardization)**。整个过程分为三步：

1.  **查看原始数据**：
    前两行代码打印出原始数据 `X` 中“温度”和“时长”这两个特征在归一化之前的最大值和最小值，让你了解它们原始的数值范围。

2.  **学习并应用归一化**：
    *   `norm_l = tf.keras.layers.Normalization(...)`：创建一个 Keras 的归一化层，这个层可以学习数据的均值和标准差。
    *   `norm_l.adapt(X)`：这是最关键的一步。该层会“学习”`X` 数据中每一列（每个特征）的平均值和标准差，并把它们记下来。
    *   `Xn = norm_l(X)`：将上面学到的规则（均值和标准差）应用到 `X` 上，计算出归一化后的新数据 `Xn`。计算公式为 `(X - 平均值) / 标准差`。

3.  **查看归一化后结果**：
    最后两行代码打印出新数据 `Xn` 的最大值和最小值，用来展示归一化后的效果。通常，归一化后的数据均值接近0，标准差接近1。

**一句话总结：**

**这串代码先展示了原始数据的数值范围，然后通过一个归一化层学习数据的统计特性，并用它将数据转换到一个标准的、更适合模型训练的范围内，最后再展示转换后的结果。**

# `np.tile`
```
Xt = np.tile(Xn,(1000,1))
Yt= np.tile(Y,(1000,1))   
print(Xt.shape, Yt.shape)   
```
- 这行代码的作用是数据扩增：将原始数据集 Xn 和 Y 垂直复制1000遍，以创建一个更大的训练集。
- 详细一点说：
  * np.tile(A, (1000, 1)) 的意思就是把数组 A 沿着第一个维度（行）重复1000次，而沿着第二个维度（列）保持不变。
  * 简单来说，就是把你的整个数据集（特征Xn和标签Y）完整地复制粘贴了999次，接在原始数据下方，从而将数据量扩大了1000倍。
  * print(Xt.shape, Yt.shape) 会显示出新数组的行数确实是原始行数的1000倍，而列数保持不变。

---

# 输入数据指定

当你像这样使用 `tf.keras.Input(shape=(2,))` 作为 `Sequential` 模型的第一项时，**就再也无需在第一个 `Dense` 全连接层中指定输入数据的形状了**。

这两种写法是等效的：

**写法一 (你的写法，现代且推荐):**

```python
model = Sequential([
    tf.keras.Input(shape=(2,)),  # 显式定义模型的输入形状
    Dense(3, activation='sigmoid', name='layer1'),
    Dense(1, activation='sigmoid', name='layer2')
])
```
*   **解释**: `tf.keras.Input` 创建了一个符号化的“输入层”，它不进行任何计算，只是告诉模型：“你将会接收到形状为 (2,) 的数据”。 后面的 `Dense` 层会自动从这个输入层推断出它所需要的输入维度，并构建相应大小的权重矩阵。

**写法二 (传统写法):**

```python
model = Sequential([
    Dense(3, activation='sigmoid', input_shape=(2,), name='layer1'), # 在第一个实际层中定义输入形状
    Dense(1, activation='sigmoid', name='layer2')
])
```
*   **解释**: 这里没有独立的输入层，而是直接在第一个计算层（`Dense`层）通过 `input_shape=(2,)` 参数来告知模型输入数据的形状。

**总结:**

使用 `tf.keras.Input` 是更现代、更清晰的做法，它将模型的“接口定义”和“计算层”分离开来。尤其是在构建更复杂的模型（例如使用函数式API构建多输入或多输出模型）时，`tf.keras.Input` 是必须的。所以，你的写法是一个非常好的习惯。

---


# `model.summary()` 参数量计算

![alt text](image-40.png)

这个计算基于一个核心公式：

**一个全连接层 (Dense Layer) 的参数数量 = (输入特征数 × 当前层神经元数) + 当前层神经元数**

这里的 `(输入特征数 × 当前层神经元数)` 是权重 (weights) 的数量，而 `当前层神经元数` 是偏置 (biases) 的数量，因为每个神经元都有一个自己的偏置项。

现在我们来分解计算图中的两个层：

### 1. **layer1 (Dense)**

*   **输入特征数**: 2 (来自 `tf.keras.Input(shape=(2,))`)
*   **当前层神经元数**: 3 (来自 `Dense(3, ...)`)
*   **计算**:
    *   权重数量 = `2 * 3 = 6`
    *   偏置数量 = `3`
    *   **总参数 = 6 + 3 = 9**

这与 `model.summary()` 中 `layer1` 对应的 `Param #` 为 9 完全吻合。

### 2. **layer2 (Dense)**

*   **输入特征数**: 3 (这层的输入是上一层 `layer1` 的输出。从 `Output Shape` 列可以看到，`layer1` 的输出形状是 `(None, 3)`，所以有3个特征)
*   **当前层神经元数**: 1 (来自 `Dense(1, ...)`)
*   **计算**:
    *   权重数量 = `3 * 1 = 3`
    *   偏置数量 = `1`
    *   **总参数 = 3 + 1 = 4**

这也与 `model.summary()` 中 `layer2` 对应的 `Param #` 为 4 完全吻合。

### 总参数 (Total params)

最后，总参数就是所有层的参数之和：

**Total params = (layer1 的参数) + (layer2 的参数) = 9 + 4 = 13**

### 关于 `Note 2` 的补充说明

截图中 `Note 2` 提到：“在最后一层包含 sigmoid 激活函数不是最佳实践”。

这是因为将 `sigmoid` 函数和损失函数（如二元交叉熵）分开计算可能会导致数值不稳定（例如，当输入非常大或非常小时，浮点数精度会出问题）。

更好的做法是：
1.  在最后一层 **不设置激活函数** (`activation=None`)，让它直接输出原始的逻辑值（logits）。
2.  在编译模型 `model.compile()` 时，选择一个能够接收 logits 的损失函数，例如 `tf.keras.losses.BinaryCrossentropy(from_logits=True)`。

这样做会把 `sigmoid` 运算和损失计算合并在一起，用一种数值上更稳定的方式来完成，从而提高模型的训练效果和稳定性。

---
