# **权重 (weights) 的随机初始化**。

```python
X_train = np.array([[1.0], [2.0]], dtype=np.float32)           
Y_train = np.array([[300.0], [500.0]], dtype=np.float32)      
```
![alt text](image-38.png)


这个 `-0.43` 不是凭空出现的，它是线性层 (linear_layer) 进行数学计算后得到的结果。这个计算遵循一个经典的公式：

**`输出 = (输入 * 权重) + 偏置`**
(`output = (input * weight) + bias`)

现在，我们来一步步分解，看看在您的例子中这几个值分别是什么：

### 1. 输入 (Input)

这个很明确，就是您传入函数的值：
*   `X_train[0]` 是 `[1.0]`。
*   经过 `.reshape(1,1)` 后，输入变成了 `[[1.0]]`。
*   所以，**输入值是 `1.0`**。

### 2. 权重 (Weight)

这是理解 `-0.43` 来源的**关键**！
*   看图片中的文字描述：“**These weights are randomly initialized to small numbers...**” (这些权重被随机初始化为一些小的数值)。
*   当您第一次调用 `linear_layer` 时，TensorFlow 就在内部创建了它的权重变量 `w`。它不知道正确的权重应该是多少，所以它就**随机猜一个很小的数**作为起点。
*   在您**这一次**运行中，TensorFlow 随机选择的那个数字**恰好就是 `-0.43`**。
*   所以，**权重 `w` 的值是 `-0.43`**。

> **重要提示**：如果您从头开始重新启动并运行这个 Notebook，权重会再次被随机初始化，您可能会得到一个完全不同的数字，比如 `0.15` 或者 `-0.81` 等等。

### 3. 偏置 (Bias)

*   图片中的文字描述说：“**...and the bias defaults to being initialized to zero.**” (...并且偏置默认被初始化为零)。
*   所以，**偏置 `b` 的值是 `0`**。

### 让我们把它们代入公式

现在我们有了所有的部分，可以进行计算了：

`输出 = (输入 * 权重) + 偏置`
`a1 = (1.0 * -0.43) + 0`
`a1 = -0.43`

**这就是 `-0.43` 这个数字的由来。**

### 总结与延伸

您可以把神经网络的训练过程想象成这样：

1.  **随机猜测**：网络最开始对权重（比如这里的 `-0.43`）进行一次完全随机的猜测。
2.  **计算结果**：用这个随机的权重进行一次计算，得到一个初步的结果（比如 `-0.43`）。
3.  **比较差距**：拿这个结果 (`-0.43`) 和我们期望的真实目标 (`Y_train` 中的 `300.0`) 进行比较，发现差距巨大。
4.  **学习和调整**：神经网络通过一个叫做“反向传播”和“梯度下降”的过程，微调它的权重，让下一次的计算结果能离真实目标更近一点。
5.  **重复**：不断重复第 2-4 步成千上万次，权重会从最初随机的 `-0.43` 被逐渐调整到一个最优的值，使得输入 `1.0` 时，输出能非常接近 `300.0`。
   

# **布尔索引 (Boolean Indexing)** 或 **掩码 (Masking)**

### **如何根据标签（或条件）来筛选和可视化数据**  
- 示例代码如下：
```
# 创建数据集：特征和标签
X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)  
Y_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1) 

pos = Y_train == 1
neg = Y_train == 0
X_train[pos]
```

### **第二部分：最关键的逻辑 - 布尔索引**

这是您可能感到困惑的核心。这个操作叫做 **布尔索引 (Boolean Indexing)** 或 **掩码 (Masking)**。

```python
pos = Y_train == 1
neg = Y_train == 0
```

1.  `pos = Y_train == 1` 是什么意思？
    *   它不是一个普通的赋值语句，而是一个**比较操作**。
    *   NumPy会逐个元素地将 `Y_train` 中的值与 `1` 进行比较。
    *   如果元素**等于** `1`，就在对应位置返回 `True`。
    *   如果元素**不等于** `1`，就在对应位置返回 `False`。

    我们来看一下具体的计算过程：
    *   `Y_train` 是: `[[0.], [0.], [0.], [1.], [1.], [1.]]`
    *   `Y_train == 1` 的结果是: `[[False], [False], [False], [True], [True], [True]]`
    *   所以，变量 `pos` 现在就是一个由 `True` 和 `False` 组成的**布尔数组 (boolean array)**。

2.  `X_train[pos]` 是什么意思？
    *   这行代码的意思是：“**从 `X_train` 中，只取出那些在 `pos` 数组中对应位置为 `True` 的元素**”。
    *   `pos` 数组在第3、4、5个位置（索引从0开始）是 `True`。
    *   所以，它就会去 `X_train` 中取出第3、4、5个位置的元素。
    *   `X_train` 中对应位置的元素是 `3.`、`4.` 和 `5.`。
    *   因此，`X_train[pos]` 的输出就是 `array([3., 4., 5.])`。

同理，`neg = Y_train == 0` 会得到 `[[True], [True], [True], [False], [False], [False]]`，所以 `X_train[neg]` 会取出 `X_train` 中前三个元素 `[0., 1., 2.]`。

---

### **将逻辑应用于绘图**

现在我们理解了 `pos` 和 `neg` 是两个“筛选器”，再来看绘图代码就非常清晰了。

1.  **绘制正例 (Positive Cases)**:
    ```python
    ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label="y=1")
    ```
    *   `X_train[pos]`: 只取出所有 `y=1` 的点的 x 坐标 `[3, 4, 5]`。
    *   `Y_train[pos]`: 只取出所有 `y=1` 的点的 y 坐标 `[1, 1, 1]`。
    *   所以这行代码画出了三个点：(3, 1), (4, 1), (5, 1)。
    *   `marker='x'` 表示用 'x' 形状来画，`c='red'` 表示用红色。

2.  **绘制负例 (Negative Cases)**:
    ```python
    ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label="y=0", ...)
    ```
    *   `X_train[neg]`: 只取出所有 `y=0` 的点的 x 坐标 `[0, 1, 2]`。
    *   `Y_train[neg]`: 只取出所有 `y=0` 的点的 y 坐标 `[0, 0, 0]`。
    *   所以这行代码画出了三个点：(0, 0), (1, 0), (2, 0)。
    *   `marker='o'` 表示用 'o' 形状来画，颜色是蓝色。

![alt text](image-39.png)

--- 

# `keras.layers.Dense` 参数解析

`keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')`

这行代码在 Keras 中定义了一个全连接层（Dense layer）。下面是每个参数的详细解释：

*   **`Dense`**: `Dense` 是 Keras 中用来创建全连接层的类。在全连接层中，每个神经元都与前一层的所有神经元相连接。 这个层实现了一个基本的数学运算：`output = activation(dot(input, kernel) + bias)`，其中 `kernel` 是权重矩阵，`bias` 是偏置向量，`activation` 是激活函数。

*   **`1` (第一个参数 `units`)**: 这个参数定义了该层中神经元（或称为单元）的数量。 在这个例子中，`units=1` 表示该层只有一个神经元。这个值决定了该层的输出空间的维度。

*   **`input_dim=1`**: 这个参数指定了输入数据的维度。 `input_dim=1` 意味着输入到这一层的数据是一个一维向量。**注意：** `input_dim` 或 `input_shape` 参数通常只需要在模型的第一层设置，因为后续的层可以自动推断出输入的形状。

*   **`activation = 'sigmoid'`**: 这个参数设置了该层神经元的激活函数。激活函数的作用是向网络中引入非线性，使得神经网络能够学习和表示更复杂的模式。
    *   **Sigmoid 函数**: Sigmoid 函数会将任何实数值压缩到 0 和 1 之间。 这使得它特别适用于二元分类问题的输出层，因为输出可以被解释为概率。

*   **`name='L1'`**: 这个参数为该层指定了一个唯一的名称 "L1"。在构建复杂的网络模型时，为层命名是一个好习惯，这有助于后续的模型可视化、调试和层操作。

--- 

# 特征归一化 Feature Normalization
```
print(f"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}")
print(f"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}")
norm_l = tf.keras.layers.Normalization(axis=-1)
norm_l.adapt(X)  # learns mean, variance
Xn = norm_l(X)
print(f"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}")
print(f"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}")
```
这段代码的核心作用是对数据进行 **特征归一化 (Feature Normalization)**，具体来说是 **标准化 (Standardization)**。整个过程分为三步：

1.  **查看原始数据**：
    前两行代码打印出原始数据 `X` 中“温度”和“时长”这两个特征在归一化之前的最大值和最小值，让你了解它们原始的数值范围。

2.  **学习并应用归一化**：
    *   `norm_l = tf.keras.layers.Normalization(...)`：创建一个 Keras 的归一化层，这个层可以学习数据的均值和标准差。
    *   `norm_l.adapt(X)`：这是最关键的一步。该层会“学习”`X` 数据中每一列（每个特征）的平均值和标准差，并把它们记下来。
    *   `Xn = norm_l(X)`：将上面学到的规则（均值和标准差）应用到 `X` 上，计算出归一化后的新数据 `Xn`。计算公式为 `(X - 平均值) / 标准差`。

3.  **查看归一化后结果**：
    最后两行代码打印出新数据 `Xn` 的最大值和最小值，用来展示归一化后的效果。通常，归一化后的数据均值接近0，标准差接近1。

**一句话总结：**

**这串代码先展示了原始数据的数值范围，然后通过一个归一化层学习数据的统计特性，并用它将数据转换到一个标准的、更适合模型训练的范围内，最后再展示转换后的结果。**

# `np.tile`
```
Xt = np.tile(Xn,(1000,1))
Yt= np.tile(Y,(1000,1))   
print(Xt.shape, Yt.shape)   
```
- 这行代码的作用是数据扩增：将原始数据集 Xn 和 Y 垂直复制1000遍，以创建一个更大的训练集。
- 详细一点说：
  * np.tile(A, (1000, 1)) 的意思就是把数组 A 沿着第一个维度（行）重复1000次，而沿着第二个维度（列）保持不变。
  * 简单来说，就是把你的整个数据集（特征Xn和标签Y）完整地复制粘贴了999次，接在原始数据下方，从而将数据量扩大了1000倍。
  * print(Xt.shape, Yt.shape) 会显示出新数组的行数确实是原始行数的1000倍，而列数保持不变。

---

# 输入数据指定

当你像这样使用 `tf.keras.Input(shape=(2,))` 作为 `Sequential` 模型的第一项时，**就再也无需在第一个 `Dense` 全连接层中指定输入数据的形状了**。

这两种写法是等效的：

**写法一 (你的写法，现代且推荐):**

```python
model = Sequential([
    tf.keras.Input(shape=(2,)),  # 显式定义模型的输入形状
    Dense(3, activation='sigmoid', name='layer1'),
    Dense(1, activation='sigmoid', name='layer2')
])
```
*   **解释**: `tf.keras.Input` 创建了一个符号化的“输入层”，它不进行任何计算，只是告诉模型：“你将会接收到形状为 (2,) 的数据”。 后面的 `Dense` 层会自动从这个输入层推断出它所需要的输入维度，并构建相应大小的权重矩阵。

**写法二 (传统写法):**

```python
model = Sequential([
    Dense(3, activation='sigmoid', input_shape=(2,), name='layer1'), # 在第一个实际层中定义输入形状
    Dense(1, activation='sigmoid', name='layer2')
])
```
*   **解释**: 这里没有独立的输入层，而是直接在第一个计算层（`Dense`层）通过 `input_shape=(2,)` 参数来告知模型输入数据的形状。

**总结:**

使用 `tf.keras.Input` 是更现代、更清晰的做法，它将模型的“接口定义”和“计算层”分离开来。尤其是在构建更复杂的模型（例如使用函数式API构建多输入或多输出模型）时，`tf.keras.Input` 是必须的。所以，你的写法是一个非常好的习惯。

---


# `model.summary()` 参数量计算

![alt text](image-40.png)

这个计算基于一个核心公式：

**一个全连接层 (Dense Layer) 的参数数量 = (输入特征数 × 当前层神经元数) + 当前层神经元数**

这里的 `(输入特征数 × 当前层神经元数)` 是权重 (weights) 的数量，而 `当前层神经元数` 是偏置 (biases) 的数量，因为每个神经元都有一个自己的偏置项。

现在我们来分解计算图中的两个层：

### 1. **layer1 (Dense)**

*   **输入特征数**: 2 (来自 `tf.keras.Input(shape=(2,))`)
*   **当前层神经元数**: 3 (来自 `Dense(3, ...)`)
*   **计算**:
    *   权重数量 = `2 * 3 = 6`
    *   偏置数量 = `3`
    *   **总参数 = 6 + 3 = 9**

这与 `model.summary()` 中 `layer1` 对应的 `Param #` 为 9 完全吻合。

### 2. **layer2 (Dense)**

*   **输入特征数**: 3 (这层的输入是上一层 `layer1` 的输出。从 `Output Shape` 列可以看到，`layer1` 的输出形状是 `(None, 3)`，所以有3个特征)
*   **当前层神经元数**: 1 (来自 `Dense(1, ...)`)
*   **计算**:
    *   权重数量 = `3 * 1 = 3`
    *   偏置数量 = `1`
    *   **总参数 = 3 + 1 = 4**

这也与 `model.summary()` 中 `layer2` 对应的 `Param #` 为 4 完全吻合。

### 总参数 (Total params)

最后，总参数就是所有层的参数之和：

**Total params = (layer1 的参数) + (layer2 的参数) = 9 + 4 = 13**

### 关于 `Note 2` 的补充说明

截图中 `Note 2` 提到：“在最后一层包含 sigmoid 激活函数不是最佳实践”。

这是因为将 `sigmoid` 函数和损失函数（如二元交叉熵）分开计算可能会导致数值不稳定（例如，当输入非常大或非常小时，浮点数精度会出问题）。

更好的做法是：
1.  在最后一层 **不设置激活函数** (`activation=None`)，让它直接输出原始的逻辑值（logits）。
2.  在编译模型 `model.compile()` 时，选择一个能够接收 logits 的损失函数，例如 `tf.keras.losses.BinaryCrossentropy(from_logits=True)`。

这样做会把 `sigmoid` 运算和损失计算合并在一起，用一种数值上更稳定的方式来完成，从而提高模型的训练效果和稳定性。

---

# 权重维度

**因为每个神经元都需要与所有的输入特征相连接，而权重矩阵的每一 *列* 都代表一个神经元所对应的权重。**


### 核心思想：矩阵乘法

一个全连接层的核心运算是矩阵乘法：`output = input • W + b`。我们先关注 `input • W` 这一部分。

假设我们只处理 **一个** 数据样本。因为你的输入数据是N行2列（N个样本，每个样本2个特征），所以一个样本可以表示为一个行向量 `[x1, x2]`，它的形状是 `(1, 2)`。

第一层有 **3个神经元**，这意味着我们希望为这一个输入样本计算出 **3个输出值**（每个神经元一个），所以我们期望的输出向量形状是 `(1, 3)`，可以表示为 `[z1, z2, z3]`。

现在，我们把这些形状代入矩阵乘法的规则中：

`input • W = output`
`(1, 2) • (W的形状) = (1, 3)`

根据矩阵乘法法则：
1.  第一个矩阵的 **列数** 必须等于 第二个矩阵的 **行数**。
2.  结果矩阵的形状是 (第一个矩阵的行数, 第二个矩阵的列数)。

应用这些规则：
1.  `input` 的列数是 `2`，所以 `W` 的行数必须是 `2`。
2.  我们期望的 `output` 的列数是 `3`，所以 `W` 的列数必须是 `3`。

**因此，权重矩阵 `W` 的形状必须是 `(2, 3)`。**

### 从单个神经元的角度理解

让我们换个角度，从每个神经元的工作方式来看，这样会更直观。

*   **第一个神经元 (产生输出 z1)**: 它需要接收 `x1` 和 `x2` 两个输入。为了计算它的输出，它需要两个对应的权重，我们称之为 `w11` 和 `w21`。
    `z1 = (x1 * w11) + (x2 * w21) + b1`

*   **第二个神经元 (产生输出 z2)**: 它也需要接收 `x1` 和 `x2` 两个输入，并拥有自己的一套权重，我们称之为 `w12` 和 `w22`。
    `z2 = (x1 * w12) + (x2 * w22) + b2`

*   **第三个神经元 (产生输出 z3)**: 同理，它的权重是 `w13` 和 `w23`。
    `z3 = (x1 * w13) + (x2 * w23) + b3`

现在，我们把这些权重整理成一个矩阵 `W`。为了能方便地进行矩阵运算，我们把每个神经元的权重组合成 **一列**：

```
        神经元1  神经元2  神经元3
       (的权重) (的权重) (的权重)
       --------------------------
来自x1: [ w11,    w12,    w13 ]
来自x2: [ w21,    w22,    w23 ]
```

这个矩阵的形状正是 **(2, 3)**！

*   **行 (Rows)**: 对应于 **输入的特征**。有2个输入特征，所以有2行。
*   **列 (Columns)**: 对应于 **当前层的神经元**。有3个神经元，所以有3列。

### 总结

所以，`W1` 的形状是 `(2, 3)` 是因为：
*   **2** 代表 `layer1` 的每个神经元都连接到来自前一层（即输入层）的 **2** 个特征。
*   **3** 代表 `layer1` 本身有 **3** 个神经元。

而 `b1` (偏置) 的形状是 `(3,)`，因为3个神经元中每一个都有一个自己独立的偏置项。

当你用 `model.get_layer("layer1").get_weights()` 获取权重时，它返回的正是这个 `(2, 3)` 的权重矩阵 `W1` 和 `(3,)` 的偏置向量 `b1`。

---

# epoch batch steps

*   **Epoch（轮次）**: 一轮是指整个训练数据集在神经网络中完成一次完整的前向传播和反向传播的过程。在您的代码中，`epochs=10` 表示模型将重复这个过程10次。
*   **Batch（批次）**: 由于一次性将所有数据输入模型进行训练会占用大量内存，通常会将训练数据分成许多个小的“批次”。
*   **Step（步数）**: 一个“步”是指模型处理完一个批次的数据并更新一次其内部参数（权重和偏置）的过程。

因此，`6250/6250` 的格式表示在这一轮（Epoch）训练中，总共有 6250 个步数（即6250个批次），而当前已经完成了全部 6250 个。

这个数字通常由以下公式决定：
`步数 = 训练样本总数 / 每个批次的大小 (batch_size)`

例如，如果您的训练集有 200,000 个样本，而批次大小（`batch_size`）为32，那么每个轮次的步数就是 `200,000 / 32 = 6250`。 在 `model.fit()` 函数中，如果没有明确指定 `steps_per_epoch`，Keras 会根据数据集大小和批次大小自动计算这个值。

![epoch batch step](image-41.png)

---

# **咖啡豆烘焙模型**

#### **目标：**
根据两个输入特征——**烘焙温度 (Temperature)** 和 **烘焙时长 (Duration)**——自动判断一批咖啡豆是“**好的烘焙 (Good Roast)**”还是“**不良烘焙 (Bad Roast)**”。

### **1. 数据输入 (The Raw Ingredients)**

一切始于最原始的数据。我们将一次具体烘焙的两个数值输入到网络中：
*   **输入 1**: 温度值 (例如：220.5 °C)
*   **输入 2**: 时长值 (例如：12.7 分钟)


### **2. 第一层 (Layer 1)**

输入的数据首先被送到第一层（Layer 1）。这一层有三个独立的神经元（单元0, 1, 2,每个神经元只关心一个特定的特征）。

*   **Unit 0**:
    *   **职责**: 只检查温度是否过低。
    *   **行为**: 如果温度太低，它就会被**高度激活**，输出一个接近 **1.0** 的大值。如果温度合适或偏高，它就保持沉默，输出一个接近 **0.0** 的小值。

*   **Unit 1**:
    *   **职责**: 只检查烘焙时长是否过短。
    *   **行为**: 如果时长太短，它就会被**高度激活**，输出一个接近 **1.0** 的大值。否则，它就保持沉默，输出一个接近 **0.0** 的小值。

*   **Unit 2**: 
    *   **职责**: 检查温度和时长的组合是否不合理（例如，温度和时长都偏低）。
    *   **行为**: 如果它发现这种不匹配的组合，它就会被**高度激活**，输出一个接近 **1.0** 的大值。否则，它就保持沉默，输出一个接近 **0.0** 的小值。

**本阶段小结：**
第一层是 **“问题发现层”**。它的任何一个神经元输出一个**大值**，都意味着检测到了一个可能导致“不良烘焙”的**危险信号**。

### **3. 第二层 (Layer 2)**

三个0到1之间的输出值被同时提交给了第二层（Layer 2），也就是最终的输出单元，并计算出这次烘焙是“**好的烘焙**”的**概率**。

为了完成这个任务，`Layer 2` 在训练中学会了一条至关重要的、**反向的决策逻辑**：

*   **决策规则**:
    *   “如果收到了来自**任何一位**检验员的**大值**报告（危险信号），那就意味着这次烘焙肯定有问题。因此，必须给出一个非常**小**的最终分数，表示它是‘好的烘焙’的概率极低。”
    *   “**只有当**收到的**所有三份**报告都是**小值**，才能放心地给出一个非常**大**的最终分数，表示它是‘好的烘焙’的概率很高。”

**本阶段小结：**
第二层是**“决策整合层”**。它将第一层的**危险信号（大值）**，转化为对“好的烘焙”的**低概率（小值）**。它是一个**逆向思维**的决策者。

### **4. 最终分类盖章 (The Final Verdict)**

`Layer 2` 给出了一个最终的概率分数（例如 0.92 或者 0.15）。但我们通常需要一个明确的“是”或“否”的结论。

这时，我们应用一个简单的**决策阈值 (Decision Threshold)**，就像盖上“合格”或“不合格”的印章一样。

*   **盖章规则 (由代码 `yhat = (fwb > 0.5).astype(int)` 定义)**:
    *   如果最终的概率分数 **大于 0.5**，我们就给它盖上“**预测为好的烘焙 (Predicted Good Roast)**”的标签。
    *   如果最终的概率分数 **小于或等于 0.5**，我们就给它盖上“**不良烘焙 (Bad Roast)**”的标签。

### **流程总结表**

| 阶段 | 角色 | “大值” (接近1.0) 的含义 | “小值” (接近0.0) 的含义 |
| :--- | :--- | :--- | :--- |
| **Layer 1** | 问题检验员 | **检测到问题！** (例如温度太低) | 一切正常，未发现此问题 |
| **Layer 2** | 品控总监 | **“好的烘焙”的概率很高** | **“好的烘焙”的概率很低** |
| **最终决策** | 分类器 | 最终判定为 **“好的烘焙”** | 最终判定为 **“不良烘焙”** |

通过这个四步流程，神经网络就成功地将两个原始的数字输入，转化成了一个有意义的、自动化的分类决策。
